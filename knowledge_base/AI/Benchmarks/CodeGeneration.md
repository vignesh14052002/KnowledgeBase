## LiveCodeBench : AI Competitive programming benchmark
[paper](https://arxiv.org/pdf/2403.07974)
[blog](https://huggingface.co/blog/leaderboard-livecodebench)
- solve competitive problems
  - Input : Problem statement from Codeforces.
  - Output : Code solution.
  - Evaluation :
    - Run Codeforces test cases.
    - Self Repair : if code fails and feedback is provided, the ability to fix code.
## Groundtruth preparation
- test cases are generated by Gpt-4-turbo based on problem description
  - verified by running on known solution

## SWE Bench : AI Software engineer benchmark
[site](https://www.swebench.com/),[paper](https://arxiv.org/pdf/2310.06770v2)
- resolve github issues
  - Input : Issue, Code base snapshot
  - Output : PR (Modified Code base)
  - Evaluation : pre configured unit tests
### Groundtruth preparation
- 2294 problems took from github issues of 12 popular python repos (django, flask, matplotlib etc)
- Benchmark construction pipeline (getting high quality PR)
  - Extract Closed PR and Issue that is being addressed 
  - Only include PRs that modifies test cases
  - Only include PRs where the closed PR's snapshot is in a runnable state without any errors
  - Filter by increase in test fail-to-pass ratio
  - After filtering, out of 90000 problems, 2294 were selected
- [openai](https://openai.com/index/introducing-swe-bench-verified) partnered to verify the benchmark
  - makes sure the tests captures that the Issue is fixed and are not dependent on the implementation details (follows BDD)
  
## SWE Lancer : AI Freelancer benchmark
[paper](https://arxiv.org/pdf/2502.12115)
- a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at $1 million USD total in realworld payouts.
  - Input/Output same as SWE bench
  - Evaluation : End to End browser automation tests from original freelancer of the task.
