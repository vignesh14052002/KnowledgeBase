## Result decoding in LLM
- [Blog](https://medium.com/@aalokpatwa/llm-decoding-balancing-quality-and-latency-23632cc0277e)
- greedy decoding
    - choose the token with the highest probability
    - cons
        - may be a local maxima
        - repetitive completions
- beam search
    - keep track of k best completions
    - cons
        - slower, high compute
- nucleus sampling
    - calculate probability distribution by softmax with temperature
    - sort the tokens by probability in descending order
    - collect tokens until the cumulative probability is less than a threshold
    - randomly sample from the collected tokens
- speculative decoding
    - use smaller model (draft model) to generate completions
    - use larger model to verify completions in single pass
    - Example:
        - draft model - gpt-3 : Time to predict token - 1s
        - larger model - gpt-4 : Time to predict token - 2s
        - draft model token generation for each iteration - 3
        - output tokens count - 12
        - time for larger model alone - 12 * 2 = 24s
        - for 1 iteration
            - draft model generation - 3 * 1 = 3s
            - larger model verification (parallel) - 2 * 1 = 2s
            - total time - 5s
        - best case : all guess from draft model are correct
            - valid tokens per iteration - 3
            - total iterations - 12/3 = 4
            - total time - 4 * 5 = 20s (saved 4s)
        - worst case: all guess from draft model are wrong
            - valid tokens per iteration - 1 (from larger model)
            - total iterations - 12/1 = 12
            - total time - 12 * 5 = 60s (extra 36s)

- medusa
    - explore https://github.com/FasterDecoding/Medusa
    - [video](https://youtu.be/Jjjn-J9SJ1s?si=Ky-Y44_-56Vm3aqY)
    - no draft model, instead use more heads in the model
    - yet to understand more
