
Learn 
- Recurrent neural networks 
- Generative adversarial networks 
- LSTM ,GRU 
- Transformers neural networks 
- BERT,GPT,T5 
- Word embedding 
- back propagation

Foreign terms 
- cross entropy error 
- vanishing gradient 

Doubts 
- can multiple data be passed on NN at same time on GPU , if possible then how the weights can be adjusted at same time , if not possible then how chatgpt gets trained on our data ( multiple users will be using at same time )
Course 
- bigram language modelling
- why log likelihood 
- use of one-hot vector 
- softmax activation 
- gradients for back propagation 
- regularisation 
